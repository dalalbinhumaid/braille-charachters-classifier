{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## Dataset\n",
    "Retrieved from [Shanks' Braille Charachters Datase](https://www.kaggle.com/shanks0465/braille-character-dataset) which consists of 60 images for each alphabet. Thus, a total of **1560** images.\n",
    "### Dataset Description\n",
    "**Title:** \n",
    "> Braille Character Dataset\n",
    "\n",
    "**Description:** \n",
    "> This dataset was created for the purpose of training a CNN for Braille Character Recognition.\n",
    "\n",
    "**Image Description:** \n",
    "> Each image is a 28x28 image in BW Scale.\n",
    "> Each image name consists of the character alphabet and the number of the image \n",
    "> and the type of data augmentation it went through. (i.e whs - width height shift, rot - Rotation, dim - brightness)\n",
    "\n",
    "**Dataset composition:**\n",
    "> 26 characters * 3 Augmentations * 20 different images of different augmentation values (i.e different shift,rotational and brightness values.)\n",
    "## Approach\n",
    "## Refrences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Activation, Dense, Input, SeparableConv2D, Conv2D, MaxPooling2D, GlobalMaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model, load_model, Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation\n",
    "Since the data was already cleaned up and pre-processesed. The following code splits the images into two generators a training and a validation generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1404 images belonging to 26 classes.\n",
      "Found 156 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "data = ImageDataGenerator(rotation_range=10, shear_range=10, validation_split=0.1)\n",
    "training_images = data.flow_from_directory('./images/', target_size=(28,28), subset='training')\n",
    "validation_images = data.flow_from_directory('./images/', target_size=(28, 28), subset='validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotImages(images):\n",
    "    fig, axes = plt.subplots(1,10, figsize=(28,28))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip(images, axes):\n",
    "        ax.imshow(img)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(10, 10))\n",
    "# for images, labels in train_ds.take(1):\n",
    "#     for i in range(9):\n",
    "#         ax = plt.subplot(3, 3, i + 1)\n",
    "#         plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "#         plt.title(int(labels[i]))\n",
    "#         plt.axis(\"off\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 1404\n",
      "Number of batches: 32\n",
      "Number of images in a batch: 44\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_batch, label = training_images.next()\n",
    "print(\"Total training images:\", training_images.n)\n",
    "print(\"Number of batches:\", image_batch.shape[0])\n",
    "print(\"Number of images in a batch:\", len(training_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x186a66ba5b0>"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "braille_a = image.load_img('./images/z/z1.JPG0rot.jpg')\n",
    "plt.imshow(braille_a)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develop the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_58 (Conv2D)          (None, 26, 26, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_24 (MaxPoolin  (None, 13, 13, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 9, 9, 128)         73856     \n",
      "                                                                 \n",
      " flatten_5 (Flatten)         (None, 10368)             0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 64)                663616    \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 26)                1690      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 758,554\n",
      "Trainable params: 758,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "print(training_images.image_shape)\n",
    "# model = Sequential([Input(shape=(28, 28, 3)),\n",
    "#                     Dense(64, activation='relu'),\n",
    "#                     Dense(26, activation='softmax')\n",
    "#                     ])\n",
    "\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# early_stopping = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(28, 28, 3)))  #\n",
    "model.add(Conv2D(32, (3, 3), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(26, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "              \n",
    "early_stopping = EarlyStopping(patience=20, verbose=1)\n",
    "\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "44/44 [==============================] - 4s 73ms/step - loss: 13.1627 - accuracy: 0.1909 - val_loss: 2.0677 - val_accuracy: 0.4936\n",
      "Epoch 2/150\n",
      "44/44 [==============================] - 2s 53ms/step - loss: 1.8869 - accuracy: 0.5107 - val_loss: 1.6597 - val_accuracy: 0.5962\n",
      "Epoch 3/150\n",
      "44/44 [==============================] - 2s 49ms/step - loss: 1.4313 - accuracy: 0.6282 - val_loss: 1.3902 - val_accuracy: 0.6603\n",
      "Epoch 4/150\n",
      "44/44 [==============================] - 2s 40ms/step - loss: 1.1543 - accuracy: 0.6937 - val_loss: 1.3825 - val_accuracy: 0.6346\n",
      "Epoch 5/150\n",
      "44/44 [==============================] - 2s 38ms/step - loss: 1.0698 - accuracy: 0.7151 - val_loss: 1.1434 - val_accuracy: 0.7244\n",
      "Epoch 6/150\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.8493 - accuracy: 0.7664 - val_loss: 1.1163 - val_accuracy: 0.7372\n",
      "Epoch 7/150\n",
      "44/44 [==============================] - 2s 53ms/step - loss: 0.6331 - accuracy: 0.8127 - val_loss: 1.1243 - val_accuracy: 0.7372\n",
      "Epoch 8/150\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 0.6217 - accuracy: 0.8219 - val_loss: 0.8889 - val_accuracy: 0.7821\n",
      "Epoch 9/150\n",
      "44/44 [==============================] - 2s 39ms/step - loss: 0.5261 - accuracy: 0.8583 - val_loss: 0.9863 - val_accuracy: 0.8269\n",
      "Epoch 10/150\n",
      "44/44 [==============================] - 2s 39ms/step - loss: 0.4616 - accuracy: 0.8746 - val_loss: 0.9943 - val_accuracy: 0.7885\n",
      "Epoch 11/150\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.4162 - accuracy: 0.8832 - val_loss: 0.8383 - val_accuracy: 0.8205\n",
      "Epoch 12/150\n",
      "44/44 [==============================] - 2s 42ms/step - loss: 0.4062 - accuracy: 0.8953 - val_loss: 0.9154 - val_accuracy: 0.8205\n",
      "Epoch 13/150\n",
      "44/44 [==============================] - 2s 39ms/step - loss: 0.3134 - accuracy: 0.9088 - val_loss: 0.9893 - val_accuracy: 0.8333\n",
      "Epoch 14/150\n",
      "44/44 [==============================] - 2s 40ms/step - loss: 0.3351 - accuracy: 0.9017 - val_loss: 0.7327 - val_accuracy: 0.8333\n",
      "Epoch 15/150\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 0.2599 - accuracy: 0.9288 - val_loss: 0.7567 - val_accuracy: 0.8654\n",
      "Epoch 16/150\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.2369 - accuracy: 0.9359 - val_loss: 1.1828 - val_accuracy: 0.8141\n",
      "Epoch 17/150\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 0.2680 - accuracy: 0.9252 - val_loss: 0.6689 - val_accuracy: 0.8718\n",
      "Epoch 18/150\n",
      "44/44 [==============================] - 2s 51ms/step - loss: 0.2009 - accuracy: 0.9558 - val_loss: 0.7425 - val_accuracy: 0.8590\n",
      "Epoch 19/150\n",
      "44/44 [==============================] - 2s 40ms/step - loss: 0.1895 - accuracy: 0.9530 - val_loss: 0.8475 - val_accuracy: 0.8526\n",
      "Epoch 20/150\n",
      "44/44 [==============================] - 2s 45ms/step - loss: 0.1675 - accuracy: 0.9509 - val_loss: 0.8253 - val_accuracy: 0.8462\n",
      "Epoch 21/150\n",
      "44/44 [==============================] - 2s 50ms/step - loss: 0.1689 - accuracy: 0.9566 - val_loss: 0.7529 - val_accuracy: 0.8782\n",
      "Epoch 22/150\n",
      "44/44 [==============================] - 3s 73ms/step - loss: 0.2258 - accuracy: 0.9316 - val_loss: 0.6292 - val_accuracy: 0.8718\n",
      "Epoch 23/150\n",
      "44/44 [==============================] - 2s 52ms/step - loss: 0.1530 - accuracy: 0.9623 - val_loss: 0.8491 - val_accuracy: 0.8654\n",
      "Epoch 24/150\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.2076 - accuracy: 0.9501 - val_loss: 0.8630 - val_accuracy: 0.8462\n",
      "Epoch 25/150\n",
      "44/44 [==============================] - 2s 45ms/step - loss: 0.2174 - accuracy: 0.9430 - val_loss: 0.7028 - val_accuracy: 0.8718\n",
      "Epoch 26/150\n",
      "44/44 [==============================] - 2s 55ms/step - loss: 0.1655 - accuracy: 0.9558 - val_loss: 0.6494 - val_accuracy: 0.8846\n",
      "Epoch 27/150\n",
      "44/44 [==============================] - 2s 47ms/step - loss: 0.1320 - accuracy: 0.9637 - val_loss: 0.9004 - val_accuracy: 0.8782\n",
      "Epoch 28/150\n",
      "44/44 [==============================] - 2s 39ms/step - loss: 0.1386 - accuracy: 0.9601 - val_loss: 0.7580 - val_accuracy: 0.8910\n",
      "Epoch 29/150\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 0.1576 - accuracy: 0.9580 - val_loss: 0.8729 - val_accuracy: 0.8590\n",
      "Epoch 30/150\n",
      "44/44 [==============================] - 2s 40ms/step - loss: 0.1364 - accuracy: 0.9644 - val_loss: 0.8597 - val_accuracy: 0.8397\n",
      "Epoch 31/150\n",
      "44/44 [==============================] - 2s 40ms/step - loss: 0.0896 - accuracy: 0.9694 - val_loss: 0.6678 - val_accuracy: 0.9038\n",
      "Epoch 32/150\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 0.0914 - accuracy: 0.9729 - val_loss: 0.6508 - val_accuracy: 0.8910\n",
      "Epoch 33/150\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 0.0830 - accuracy: 0.9708 - val_loss: 0.7075 - val_accuracy: 0.8846\n",
      "Epoch 34/150\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.1852 - accuracy: 0.9566 - val_loss: 1.0558 - val_accuracy: 0.8526\n",
      "Epoch 35/150\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.1439 - accuracy: 0.9672 - val_loss: 0.8569 - val_accuracy: 0.8462\n",
      "Epoch 36/150\n",
      "44/44 [==============================] - 2s 46ms/step - loss: 0.0828 - accuracy: 0.9758 - val_loss: 0.8697 - val_accuracy: 0.8846\n",
      "Epoch 37/150\n",
      "44/44 [==============================] - 2s 43ms/step - loss: 0.0575 - accuracy: 0.9801 - val_loss: 0.7411 - val_accuracy: 0.8782\n",
      "Epoch 38/150\n",
      "44/44 [==============================] - 2s 44ms/step - loss: 0.0816 - accuracy: 0.9786 - val_loss: 0.7953 - val_accuracy: 0.8846\n",
      "Epoch 39/150\n",
      "44/44 [==============================] - 2s 39ms/step - loss: 0.0673 - accuracy: 0.9808 - val_loss: 0.8352 - val_accuracy: 0.8718\n",
      "Epoch 40/150\n",
      "44/44 [==============================] - 2s 40ms/step - loss: 0.1094 - accuracy: 0.9715 - val_loss: 1.0745 - val_accuracy: 0.8590\n",
      "Epoch 41/150\n",
      "44/44 [==============================] - 2s 41ms/step - loss: 0.1394 - accuracy: 0.9587 - val_loss: 0.8343 - val_accuracy: 0.8782\n",
      "Epoch 42/150\n",
      "44/44 [==============================] - 2s 39ms/step - loss: 0.0377 - accuracy: 0.9843 - val_loss: 1.0324 - val_accuracy: 0.8782\n",
      "Epoch 00042: early stopping\n"
     ]
    }
   ],
   "source": [
    "trained_model = model.fit(training_images,\n",
    "                          validation_data=validation_images,\n",
    "                          epochs=150,\n",
    "                          batch_size=32,\n",
    "                          callbacks=[early_stopping],\n",
    "                          verbose=1)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
